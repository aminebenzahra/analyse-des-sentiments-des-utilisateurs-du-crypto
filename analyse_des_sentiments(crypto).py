# -*- coding: utf-8 -*-
"""analyse des sentiments(crypto)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12dNzIHpb-xwYXlw0G-mIMSJ-5Be_r5yp
"""

pip install snscrape

pip install spacy

pip install textblob

pip install wordcloud

!python -m spacy download en_core_web_sm

from datetime import date
import snscrape.modules.twitter as sntwitter
import pandas as pd
import numpy as np
from textblob import TextBlob
from wordcloud import WordCloud
import re
import matplotlib.pyplot as plt
import seaborn as sns
plt.style.use('fivethirtyeight')
import nltk
from nltk.stem.snowball import SnowballStemmer
from nltk.corpus import stopwords
import spacy
nlp = spacy.load("en_core_web_sm")

#hide warnings
import warnings
warnings.filterwarnings('ignore')

# Create a list for tweet data
tweets_list = []
maxTweets = 50000

# Using TwitterSearchScraper to scrape data and append tweets to list
for i,tweet in enumerate(sntwitter.TwitterSearchScraper('crypto since:2020-01-01 until:{today}').get_items()):
    if i>maxTweets:
        break
    tweets_list.append([tweet.content])
    
# Creating a dataframe from the tweets list above
tweets_to_df = pd.DataFrame(tweets_list, columns=['Tweets'])

tweets_to_df.head() #lists first five tweets

#clean the tweets
def cleanTweets(text):
    text = re.sub('@[A-Za-z0-9_]+', '', text) #removes @mentions
    text = re.sub('#','',text) #removes hashtag '#' symbol
    text = re.sub('RT[\s]+','',text)
    text = re.sub('https?:\/\/\S+', '', text) 
    text = re.sub('\n',' ',text)
    return text
tweets_to_df['cleanedTweets'] = tweets_to_df['Tweets'].apply(cleanTweets) #apply cleanTweet function to the tweet
tweets_to_df.tail() #compares original tweets with cleaned Tweets

tweets_to_df.to_csv('crypto_tweets.csv') #write dataframe into csv file

savedTweets = pd.read_csv('crypto_tweets.csv',index_col=0) #reads csv file

savedTweets

#get subjectivity and polarity of tweets with a function
def getSubjectivity(text):
    return TextBlob(text).sentiment.subjectivity

def getPolarity(text):
    return TextBlob(text).sentiment.polarity

savedTweets['Subjectivity'] = savedTweets['cleanedTweets'].apply(getSubjectivity)
savedTweets['polarity'] = savedTweets['cleanedTweets'].apply(getPolarity)

savedTweets.head()

#create a function to check negative, neutral and positive analysis based on polarity
def getAnalysis(score):
    if score<0:
        return 'Negative'
    elif score ==0:
        return 'Neutral'
    else:
        return 'Positive'
    
savedTweets['Analysis'] = savedTweets['polarity'].apply(getAnalysis)
savedTweets.head()

savedTweets['Analysis'].value_counts() #shows the counts of tweets' polarity

#plot a bar graph to show count of tweet sentiment
fig = plt.figure(figsize=(8,5))
color = ['blue','green','orange']
savedTweets['Analysis'].value_counts().plot(kind='bar',color = color)
plt.title('Value count of tweet polarity')
plt.ylabel('Count')
plt.xlabel('Polarity')
plt.grid(False)
plt.show()

#plot the polarity and subjectivity on a scatter plot
plt.figure(figsize=(9,7))
for i in range(0,savedTweets.shape[0]):
    plt.scatter(savedTweets['polarity'][i],savedTweets['Subjectivity'][i], color='red')
plt.title('Sentiment Analysis')
plt.xlabel('polarity')
plt.ylabel('Subjectivity')
plt.show()

#create a function for wordcloud
def create_wordcloud(text):    
    allWords = ' '.join([tweets for tweets in text])
    wordCloud = WordCloud(background_color='white', width=800, height=500, random_state=22, max_font_size=140).generate(allWords)
    plt.figure(figsize=(20,10))
    plt.imshow(wordCloud)
    plt.axis('off')
    plt.show()
#wordcloud for positive tweets
posTweets = savedTweets.loc[savedTweets['Analysis']=='Positive', 'cleanedTweets']
create_wordcloud(posTweets)
#wordcloud for negative tweets
negTweets = savedTweets.loc[savedTweets['Analysis']=='Negative', 'cleanedTweets']
create_wordcloud(negTweets)

#break each tweet sentence into words
sentences = []
for x in savedTweets['cleanedTweets']:
    sentences.append(x)
sentences
lines = list()
for line in sentences:
    words = line.split()
    for w in words:
        lines.append(w)
lines[:15] #shows first 10 words in the first tweet

#stemming all the words to their root word
stemmer = SnowballStemmer(language='english')
stem=[]
for x in lines:
    stem.append(stemmer.stem(x))
stem[:20]
#removes stopwords (very common words in a sentence)
stem2 = []
for x in stem:
    if x not in nlp.Defaults.stop_words:
        stem2.append(x)
#creates a new dataframe for the stem and shows the count of the most used words
df = pd.DataFrame(stem2)
df=df[0].value_counts()
df #shows the new dataframe

#plots the top 20 used words
df = df[:15]
plt.figure(figsize=(12,7))
sns.barplot(df.values, df.index, alpha=0.8)
plt.title('Top Words Overall')
plt.xlabel('Count of words', fontsize=12)
plt.ylabel('Word from Tweet', fontsize=12)
plt.show()